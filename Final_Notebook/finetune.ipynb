{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f784d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import json\n",
    "from Bio import Entrez\n",
    "import random\n",
    "import time\n",
    "from itertools import islice\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8f4267",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\")\n",
    "TRAIN_PATH = Path(\"./training13b.json\")\n",
    "\n",
    "max_per_query = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5cdb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMAIL = \"pgreinald@gmail.com\"           # Enter your E-Mail(The API will start compaining if not entered)\n",
    "API_KEY = \"9666f51fccbd68a29320334f1d78ad257608\"         # Enter your API Key(More Queries/s if entered)\n",
    "\n",
    "Entrez.email = EMAIL\n",
    "Entrez.api_key = API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecd66f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TRAIN_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d847a2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "RETMX = 1000  # Number of documents to fetch\n",
    "TRAIN_DATA_URL  = \"https://participants-area.bioasq.org/Tasks/13b/trainingDataset/training13b.json\"\n",
    "DATA_FILE = \"training13b.json\"\n",
    "BATCH = 10_000_000    # PubMed efetch batch size\n",
    "\n",
    "# Load the Training File\n",
    "with open(DATA_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "questions = data.get(\"questions\", [])\n",
    "\n",
    "##############################---Utility Functions---###################################\n",
    "\n",
    "def ensure_training_file() -> None:\n",
    "    if os.path.exists(DATA_FILE) and os.path.getsize(DATA_FILE) > 0:\n",
    "        return\n",
    "    print(\"Downloading training data …\")\n",
    "    resp = requests.get(TRAIN_DATA_URL, timeout=30)\n",
    "    resp.raise_for_status()\n",
    "    if b\"<html\" in resp.content[:100].lower():\n",
    "        raise RuntimeError(\"Downloaded content is HTML, not JSON - check URL/login.\")\n",
    "    with open(DATA_FILE, \"wb\") as fh:\n",
    "        fh.write(resp.content)\n",
    "    print(f\"Saved → {DATA_FILE}\")\n",
    "\n",
    "def esearch_pmids(query: str, retmax: int = RETMX) -> list:\n",
    "    \"\"\"\n",
    "    Perform a PubMed search using both Title/Abstract and MeSH terms.\n",
    "    Returns a list of PMIDs (strings).\n",
    "    \"\"\"\n",
    "    # Cleaning up teh query by phrasing it as well as tokenizing it.\n",
    "    phrase = query.lower().strip()\n",
    "    tokens = phrase.split()\n",
    "    # Build query parts for full phrase and individual tokens. Mesh Terms are \n",
    "    # standardized vocabulary used to enhance search precision by capturing the semantic meaning of biomedical concepts.\n",
    "    # Frankly this really helps in the document retrival part.\n",
    "    parts = [f'\"{phrase}\"[Title/Abstract]', f'\"{phrase}\"[MeSH Terms]']\n",
    "    for t in tokens:\n",
    "        parts.append(f'{t}[Title/Abstract]')\n",
    "        parts.append(f'{t}[MeSH Terms]')\n",
    "    term = f\"({' OR '.join(parts)}) AND hasabstract[text]\"\n",
    "\n",
    "    # Retry up to 3 times on failure\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            handle = Entrez.esearch(\n",
    "                db=\"pubmed\",\n",
    "                term=term,\n",
    "                retmax=retmax,\n",
    "                sort=\"relevance\",\n",
    "                retmode=\"xml\"\n",
    "            )\n",
    "            result = Entrez.read(handle)\n",
    "            time.sleep(0.1)\n",
    "            return result.get('IdList', [])\n",
    "        except Exception as e:\n",
    "            print(f\"Esearch attempt {attempt+1} failed: {e}\")\n",
    "            time.sleep(2 ** attempt)\n",
    "    return []\n",
    "\n",
    "#####################################################################################\n",
    "\n",
    "def fetch_documents(pmids: list[str], batch_size: int = BATCH) -> list[tuple[str,str,str]]:\n",
    "    docs: list[tuple[str,str,str]] = []\n",
    "    for start in range(0, len(pmids), batch_size):\n",
    "        batch = pmids[start:start+batch_size]\n",
    "        for attempt in range(3):\n",
    "            try:\n",
    "                h = Entrez.efetch(db=\"pubmed\", id=\",\".join(batch), retmode=\"xml\")\n",
    "                recs = Entrez.read(h)\n",
    "                time.sleep(0.34)         \n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"    [efetch] attempt {attempt+1}/3 failed → {e}\")\n",
    "                time.sleep(2 ** attempt)\n",
    "        else:                            # all retries exhausted\n",
    "            continue\n",
    "\n",
    "        for art in recs.get(\"PubmedArticle\", []):\n",
    "            pmid   = art[\"MedlineCitation\"][\"PMID\"]\n",
    "            art_el = art[\"MedlineCitation\"][\"Article\"]\n",
    "            title  = art_el.get(\"ArticleTitle\", \"\")\n",
    "            abst   = \" \".join(art_el.get(\"Abstract\", {})\n",
    "                              .get(\"AbstractText\", []))\n",
    "            docs.append((pmid, title, abst))\n",
    "    return docs\n",
    "\n",
    "# Loop Iteration for each query in the training file or testin batches. \n",
    "def save_docs() -> None:\n",
    "    ensure_training_file()\n",
    "\n",
    "    # Load input questions\n",
    "    with open(DATA_FILE, \"r\", encoding=\"utf-8\") as fh:\n",
    "        questions = json.load(fh)[\"questions\"]\n",
    "\n",
    "    # Output file setup\n",
    "    out_dir  = Path(\"api_retrieval\")\n",
    "    out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        # Nice progress bar if tqdm available\n",
    "\n",
    "        # with out_path.open(mode, encoding=\"utf-8\") as fout:\n",
    "        for q in tqdm(questions, total=len(questions), unit=\"q\"):\n",
    "            qid, qtype = q[\"id\"], q[\"type\"]\n",
    "            out_path = out_dir / (qid + \".json\")\n",
    "\n",
    "            if out_path.is_file():\n",
    "                continue\n",
    "            \n",
    "            body       = q[\"body\"].strip()\n",
    "\n",
    "\n",
    "            if not out_path.is_file():\n",
    "                pmids = esearch_pmids(body)\n",
    "                docs  = fetch_documents(pmids)\n",
    "\n",
    "                record = {\n",
    "                    \"id\": qid,\n",
    "                    \"type\": qtype,\n",
    "                    \"body\": body,\n",
    "                    \"documents\": [\n",
    "                        {\"pmid\": p, \"title\": t, \"abstract\": a}\n",
    "                        for p, t, a in docs\n",
    "                    ]\n",
    "                }\n",
    "\n",
    "                with out_path.open(\"w\", encoding=\"utf-8\") as fout:\n",
    "                    fout.write(json.dumps(record))\n",
    "                    fout.flush()                   # ensure line is on disk\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nInterrupted by user.\")\n",
    "\n",
    "    print(\"\\n✅ Finished.\")\n",
    "\n",
    "save_docs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aa5881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article(pubmed_id: str, skip_sleep: bool = False) -> tuple[str, str]:\n",
    "    # Fetch the record from PubMed\n",
    "    if not skip_sleep:\n",
    "        time.sleep(0.1)      \n",
    "    handle = Entrez.efetch(db=\"pubmed\", id=pubmed_id, rettype=\"abstract\", retmode=\"xml\")\n",
    "    records = Entrez.read(handle)\n",
    "\n",
    "    # Extract title and abstract\n",
    "    try:\n",
    "        article = records['PubmedArticle'][0]['MedlineCitation']['Article']\n",
    "        title = article['ArticleTitle']\n",
    "        abstract = article['Abstract']['AbstractText'][0]\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        print(f\"The id is {pubmed_id}\")\n",
    "        print(f\"{records}\")\n",
    "        title = \"\"\n",
    "        abstract = \"\"\n",
    "\n",
    "    return title, abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2decad23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_article(title: str, abstract: str) -> str:\n",
    "    return f\"{title} {abstract}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa6803e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filter_ids(generic_ids: list[str], ground_truth_ids: list[str], limit: int = 100) -> list[str]:\n",
    "    return list(islice((id_ for id_ in generic_ids if id_ not in ground_truth_ids), limit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25706b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_examples(data: dict = data, max_per_query: int = max_per_query):\n",
    "    train_examples = []\n",
    "\n",
    "    for q in tqdm(data[\"questions\"]):\n",
    "        q_id = q[\"id\"]\n",
    "        q_query = q[\"body\"]\n",
    "        ground_truth = q[\"documents\"]\n",
    "        ground_truth = [i.split(\"/\")[4] for i in ground_truth]\n",
    "\n",
    "        ground_truth_texts = [concat_article(*get_article(id)) for id in ground_truth]\n",
    "\n",
    "        DOCS_PATH = Path(\"./api_retrieval/\" + q[\"id\"] + \".json\")\n",
    "\n",
    "        if DOCS_PATH.is_file():\n",
    "            with open(DOCS_PATH, encoding=\"utf-8\") as f:\n",
    "                docs = json.load(f)\n",
    "                docs = docs[\"documents\"]\n",
    "\n",
    "            if len(docs) <= 0:\n",
    "                continue\n",
    "            \n",
    "            neg_ids = [doc[\"pmid\"] for doc in docs if not doc[\"pmid\"] in ground_truth]\n",
    "\n",
    "            if len(neg_ids) <= 0:\n",
    "                continue\n",
    "\n",
    "            for n in range(min(len(neg_ids), max_per_query)):\n",
    "                pos_article = random.choice(ground_truth_texts)\n",
    "                neg_id = neg_ids[n]\n",
    "\n",
    "                neg_title = docs[n][\"title\"]\n",
    "                neg_abstract = docs[n][\"abstract\"]\n",
    "                    \n",
    "                neg_article = concat_article(neg_title, neg_abstract)\n",
    "                \n",
    "                train_examples.append(InputExample(texts=[q_query, pos_article, neg_article]))\n",
    "\n",
    "    return train_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929f0665",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = create_train_examples(data)\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c1aaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = losses.TripletLoss(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd7b528",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=2,\n",
    "    warmup_steps=100,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "# Save model\n",
    "model.save(\"./triplet-finetuned-BioBERT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346b13bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sbert-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
