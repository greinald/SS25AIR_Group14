{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67600a75",
   "metadata": {},
   "source": [
    "# Calculate best possible score from retrieved documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25e217ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/julian/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/julian/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Standard Library\n",
    "import json           \n",
    "import time           \n",
    "import logging        \n",
    "import string        \n",
    "import statistics\n",
    "import re             \n",
    "from pathlib import Path \n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "\n",
    "# Bioinformatics Libraries\n",
    "from Bio import Entrez, Medline   # For accessing and parsing PubMed/NCBI data\n",
    "\n",
    "# Text Search / Ranking\n",
    "from rank_bm25 import BM25Okapi   # For BM25 ranking algorithm. Extension of the TF-IDF (Term Frequency-Inverse Document Frequency) model, taking into account term frequency saturation and document length to improve ranking accuracy. \n",
    "\n",
    "\n",
    "# NLP and Tokenization Tools\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "\n",
    "from typing import List\n",
    "import string\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Progress Visualization\n",
    "from tqdm import tqdm, trange    \n",
    "\n",
    "# Machine Learning Libraries\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    AutoModelForQuestionAnswering, pipeline\n",
    ")\n",
    "\n",
    "import scipy\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "490bceac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get your API and put it HERE https://account.ncbi.nlm.nih.gov/settings/\n",
    "EMAIL   = \"julian.fluer@outlook.com\"        \n",
    "API_KEY = \"258c639ed1b7790cc1b355f7cdca25e4f109\"       \n",
    "\n",
    "# API key for synonyms\n",
    "MESH_API_KEY = \"2bb9ddd9-1828-419d-81df-bd89c4a97130\"\n",
    "BASE_URL = \"https://data.bioontology.org/search\"\n",
    "\n",
    "# hyper-params you can play around with these to see if you get different results(Only play with candidates)\n",
    "MAX_DOCS = 10 # list size required by BioASQ Phase-A\n",
    "RETMX = 10000 # retrieve more, then truncate\n",
    "SLEEP = 0.11 # 10 requests / sec with API key\n",
    "\n",
    "ROOT = Path(\"data\")\n",
    "TRAIN = ROOT / \"training13b.json\"\n",
    "OUT = ROOT / \"bm25_phaseA_run.json\"  \n",
    "CACHE_DIR = ROOT / \"medline_cache\"\n",
    "CACHE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=\"data/phaseA_esearch.log\",\n",
    "    level=logging.WARNING,\n",
    "    format=\"%(asctime)s %(levelname)s %(message)s\"\n",
    ")\n",
    "\n",
    "Entrez.email   = EMAIL\n",
    "Entrez.api_key = API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb226a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg(relevances, k):\n",
    "    \"\"\"Compute Discounted Cumulative Gain up to rank k.\"\"\"\n",
    "    relevances = np.asarray(relevances)[:k]\n",
    "    return np.sum((2**relevances - 1) / np.log2(np.arange(2, relevances.size + 2)))\n",
    "\n",
    "def ndcg_at_k(predicted_list, ground_truth_list, k = 10):\n",
    "    \"\"\"\n",
    "    Compute NDCG@k where both predicted and ground truth are ranked lists of doc IDs.\n",
    "    \n",
    "    Args:\n",
    "        predicted_list (list): List of predicted doc IDs in ranked order.\n",
    "        ground_truth_list (list): List of ground truth doc IDs in ranked order.\n",
    "        k (int): Truncation level.\n",
    "    \n",
    "    Returns:\n",
    "        float: NDCG@k score.\n",
    "    \"\"\"\n",
    "    # Assign implicit relevance scores: higher rank = higher score\n",
    "    max_relevance = len(ground_truth_list)\n",
    "    ground_truth_relevance = {\n",
    "        doc_id: max_relevance - rank for rank, doc_id in enumerate(ground_truth_list)\n",
    "    }\n",
    "\n",
    "    # Map predicted documents to their relevance based on ground truth rank\n",
    "    predicted_relevances = [ground_truth_relevance.get(doc_id, 0) for doc_id in predicted_list]\n",
    "\n",
    "    # Ideal DCG from the perfect ranking (i.e., ground truth itself)\n",
    "    ideal_relevances = sorted(ground_truth_relevance.values(), reverse=True)\n",
    "\n",
    "    dcg_score = dcg(predicted_relevances, k)\n",
    "    idcg_score = dcg(ideal_relevances, k)\n",
    "\n",
    "    if idcg_score == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return dcg_score / idcg_score\n",
    "\n",
    "def mean_ndcg_at_k(predictions, questions, k = 10):\n",
    "    sum_ndcg = 0\n",
    "\n",
    "    for prediction, question in zip(predictions, questions):\n",
    "        sum_ndcg += ndcg_at_k(prediction[\"documents\"], question[\"documents\"], k)\n",
    "\n",
    "    mean_ndcg = sum_ndcg / len(predictions)\n",
    "\n",
    "    return mean_ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a58c2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mesh_synonyms(query, limit=5):\n",
    "    params = {\n",
    "        'q': query,\n",
    "        'ontologies': 'MESH',\n",
    "        'apikey': MESH_API_KEY,\n",
    "        'include': 'synonym,prefLabel',\n",
    "        'pagesize': limit\n",
    "    }\n",
    "\n",
    "    response = requests.get(BASE_URL, params=params)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        return []\n",
    "\n",
    "    data = response.json()\n",
    "    synonyms = set()\n",
    "\n",
    "    for result in data.get('collection', []):\n",
    "        pref_label = result.get('prefLabel', '')\n",
    "        synonyms.add(pref_label.lower())\n",
    "        for syn in result.get('synonym', []):\n",
    "            synonyms.add(syn.lower())\n",
    "\n",
    "    synonyms.discard(query.lower())\n",
    "    return list(synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "290d470a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_query_with_mesh(query):\n",
    "    mesh_synonyms = get_mesh_synonyms(query)\n",
    "    terms = [f'\"{query}\"[Title/Abstract]']\n",
    "    terms += [f'\"{syn}\"[Title/Abstract]' for syn in mesh_synonyms]\n",
    "    terms += [f'\"{syn}\"[MeSH Terms]' for syn in mesh_synonyms]\n",
    "    expanded_query = ' OR '.join(terms)\n",
    "    return expanded_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b09cbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(query: str) -> (str, List[str]):\n",
    "    s = re.sub(r'[^a-z0-9\\s]', ' ', query.lower())\n",
    "    tokens = [t for t in word_tokenize(s) if len(t) >= 3]\n",
    "    phrase = \" \".join(tokens)\n",
    "    return phrase, tokens\n",
    "\n",
    "def esearch_pmids(query: str, k: int = RETMX) -> List[str]:\n",
    "    phrase, tokens = clean(query)\n",
    "    if not tokens:\n",
    "        return []\n",
    "    parts = []\n",
    "    # full‑phrase lookups\n",
    "    parts.append(f'\"{phrase}\"[Title/Abstract]')\n",
    "    parts.append(f'\"{phrase}\"[MeSH Terms]')\n",
    "    # individual‑token lookups\n",
    "    parts += [f'{t}[Title/Abstract]' for t in tokens]\n",
    "    parts += [f'{t}[MeSH Terms]' for t in tokens]\n",
    "    term = f\"({' OR '.join(parts)}) AND hasabstract[text]\"\n",
    "    for attempt in range(1, 4):\n",
    "        try:\n",
    "            handle = Entrez.esearch(\n",
    "                db=\"pubmed\",\n",
    "                term=term,\n",
    "                retmax=k,\n",
    "                sort=\"relevance\",\n",
    "                retmode=\"xml\"\n",
    "            )\n",
    "            ids = Entrez.read(handle).get(\"IdList\", [])\n",
    "            time.sleep(SLEEP)\n",
    "            return ids\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"eSearch retry {attempt}: {e!r}\")\n",
    "            time.sleep(2 ** attempt)\n",
    "    logging.error(f\"All eSearch retries failed for {query!r}\")\n",
    "    return []\n",
    "\n",
    "def pmid_to_url(pmid: str) -> str:\n",
    "    return f\"http://www.ncbi.nlm.nih.gov/pubmed/{pmid}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "643ab982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5389 questions from training13b.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5389/5389 [3:43:58<00:00,  2.49s/Q]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote Phase-A run file → /Users/julian/Documents/TU/Advanced_Information_Retrieval/project/data/bm25_phaseA_run.json\n",
      "Check phaseA_esearch.log for warnings or API errors.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "qs = json.loads(TRAIN.read_text())[\"questions\"]\n",
    "print(f\"Loaded {len(qs)} questions from {TRAIN.name}\")\n",
    "\n",
    "predictions = []\n",
    "for q in tqdm(qs, unit=\"Q\"):\n",
    "    pmids = esearch_pmids(q[\"body\"])\n",
    "    predictions.append({\n",
    "        \"id\": q[\"id\"],\n",
    "        \"documents\": [pmid_to_url(p) for p in pmids],\n",
    "        \"snippets\": []                      # keep the field, even if empty becuase the test expects it\n",
    "    })\n",
    "\n",
    "OUT.write_text(json.dumps({\"questions\": predictions}, indent=2))    \n",
    "print(f\"Wrote Phase-A run file → {OUT.resolve()}\")\n",
    "print(\"Check phaseA_esearch.log for warnings or API errors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5292446",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = json.loads(TRAIN.read_text())[\"questions\"]\n",
    "corpus = json.loads(OUT.read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d1c924",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to interrupt the Kernel. \n",
      "\u001b[1;31mThe kernel died. Error: ... View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350df602",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
