{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tabel\"></a>\n",
    "<b>Table of contents:</b>\n",
    "\n",
    "\n",
    "* [1. Import Libraries & Dataset](#import)\n",
    "\n",
    "[üè† Table of Contents](#tabel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Bio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    ".phase-container {\n",
    "    display: flex;\n",
    "    flex-wrap: wrap;\n",
    "    justify-content: center;\n",
    "    gap: 15px;\n",
    "    padding: 20px;\n",
    "    font-family: 'Poppins', Roboto, sans-serif;\n",
    "}\n",
    "\n",
    ".phase {\n",
    "    width: 260px;\n",
    "    height: 70px;\n",
    "    line-height: 70px;\n",
    "    color: white;\n",
    "    font-weight: 600;\n",
    "    text-align: center;\n",
    "    border-radius: 6px;\n",
    "    position: relative;\n",
    "    font-size: 1.05em;\n",
    "    clip-path: polygon(0% 0%, 92% 0%, 100% 50%, 92% 100%, 0% 100%);\n",
    "    box-shadow: 2px 4px 10px rgba(0, 0, 0, 0.2);\n",
    "    opacity: 0.9;\n",
    "    transition: all 0.3s ease;\n",
    "    flex-shrink: 0;\n",
    "}\n",
    "\n",
    ".phase:hover {\n",
    "    transform: scale(1.05);\n",
    "    box-shadow: 4px 6px 15px rgba(0, 0, 0, 0.25);\n",
    "    opacity: 1;\n",
    "}\n",
    "\n",
    ".phase1 { background: #69B8F7; }\n",
    ".phase2 { background: #54A6F3; }\n",
    ".phase3 { background: #2B7BC1; }\n",
    ".phase4 { background: #1F557F; }\n",
    ".phase5 { background: #5C9D99; }\n",
    ".phase6 { background: #4E9B97; }\n",
    "\n",
    "@media (max-width: 768px) {\n",
    "    .phase {\n",
    "        width: 90%;\n",
    "    }\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div class=\"phase-container\">\n",
    "    <div class=\"phase phase1\">1. Create Baseline</div>\n",
    "    <div class=\"phase phase2\">2. Add BM25 (Retrieval Algorithm)</div>\n",
    "    <div class=\"phase phase3\">3. Add Classification of Questions Model</div>\n",
    "    <div class=\"phase phase4\">4. Add Model for Yes/No Questions</div>\n",
    "    <div class=\"phase phase5\">5. Add Model for Factoid, List Questions</div>\n",
    "    <div class=\"phase phase6\">6. Add Model for Summary Questions</div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Imports\n",
    "\n",
    "\n",
    "<a id=\"import\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library\n",
    "import json           \n",
    "import time           \n",
    "import logging        \n",
    "import string        \n",
    "import statistics\n",
    "import re             \n",
    "from pathlib import Path \n",
    "from typing import List, Dict\n",
    "\n",
    "# Bioinformatics Libraries\n",
    "from Bio import Entrez, Medline   # For accessing and parsing PubMed/NCBI data\n",
    "\n",
    "# Text Search / Ranking\n",
    "from rank_bm25 import BM25Okapi   # For BM25 ranking algorithm. Extension of the TF-IDF (Term Frequency-Inverse Document Frequency) model, taking into account term frequency saturation and document length to improve ranking accuracy. \n",
    "\n",
    "\n",
    "# NLP and Tokenization Tools\n",
    "import nltk                       # Natural Language Toolkit (tokenization, stopwords, etc.)\n",
    "\n",
    "# Progress Visualization\n",
    "from tqdm import tqdm, trange    \n",
    "\n",
    "# Machine Learning Libraries\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    AutoModelForQuestionAnswering, pipeline\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insanly Slow. Just know that till I state differently what we are doing here is nothing else but just a automatic search of the database\n",
    "# Without any aditional thing added. This is so to say like a random predictor and everything for phase A will be built on top of this.\n",
    "\"\"\"\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=\"phaseA_errors.log\",\n",
    "    level=logging.WARNING,\n",
    "    format=\"%(asctime)s %(levelname)s %(message)s\"\n",
    ")\n",
    "\n",
    "\n",
    "TRAIN = Path(\"/Users/greinaldpappa/Downloads/BioASQ-training13b/training13b.json\")\n",
    "OUT   = Path(\"./api_phaseA_run.json\")\n",
    "\n",
    "\n",
    "data = json.loads(TRAIN.read_text())\n",
    "qs   = data[\"questions\"]\n",
    "print(f\"üóíÔ∏è  Loaded {len(qs)} questions.\")\n",
    "\n",
    "\n",
    "sess = start_session()\n",
    "print(\"üóùÔ∏è  API session:\", sess)\n",
    "\n",
    "\n",
    "predictions = []\n",
    "for q in tqdm.tqdm(qs, unit=\"Q\"):\n",
    "    qid, query = q[\"id\"], q[\"body\"]\n",
    "    pmids = []\n",
    "    for attempt in range(1, 4):       # up to 3 tries per question\n",
    "        try:\n",
    "            pmids = find_pubmed_citations(sess, query, k=1000)\n",
    "            break                       # success ‚Üí exit retry loop\n",
    "        except Exception as exc:\n",
    "            wait = 2 ** attempt         # exponential back-off: 2,4,8s\n",
    "            logging.warning(\n",
    "                f\"QID={qid} attempt {attempt} failed: {exc!r}; retrying in {wait}s\"\n",
    "            )\n",
    "            time.sleep(wait)\n",
    "    else:\n",
    "        # all retries failed ‚Üí log and move on with empty list\n",
    "        logging.error(f\"QID={qid} all retries failed; returning empty list\")\n",
    "    predictions.append({\n",
    "        \"query_id\": qid,\n",
    "        \"documents\": pmids\n",
    "    })\n",
    "    time.sleep(0.1)                \n",
    "\n",
    "\n",
    "OUT.write_text(json.dumps(predictions, indent=2))\n",
    "print(f\"‚úÖ  Wrote {len(predictions)} entries to {OUT}\")\n",
    "print(\"‚ö†Ô∏è  If you saw errors, check phaseA_errors.log.\")\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Information Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóíÔ∏è  Loaded 5389 questions from training13b.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5389/5389 [1:05:46<00:00,  1.37Q/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ  Wrote Phase-A run file ‚Üí /Users/greinaldpappa/Documents/GitHub/SS25AIR_Group14/esearch_phaseA_run.json\n",
      "‚ö†Ô∏è  Check phaseA_esearch.log for warnings or API errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "TRAIN = Path(\"/Users/greinaldpappa/Downloads/BioASQ-training13b/training13b.json\")\n",
    "OUT   = Path(\"/Users/greinaldpappa/Downloads/BioASQ-training13b/esearch_phaseA_run.json\")\n",
    "\n",
    "# Get your API and put it HERE https://account.ncbi.nlm.nih.gov/settings/\n",
    "EMAIL   = \"\"        \n",
    "API_KEY = \"\"       \n",
    "\n",
    "MAX_DOCS = 10        # list size required by BioASQ Phase-A\n",
    "RETMX    = 60        # retrieve more, then truncate\n",
    "SLEEP    = 0.11      # 10 requests / sec with API key\n",
    "\n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=\"/Users/greinaldpappa/Downloads/BioASQ-training13b/phaseA_esearch.log\",\n",
    "    level=logging.WARNING,\n",
    "    format=\"%(asctime)s %(levelname)s %(message)s\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "Entrez.email   = EMAIL\n",
    "Entrez.api_key = API_KEY\n",
    "\n",
    "\n",
    "\n",
    "def esearch_pmids(query: str, k: int = RETMX) -> List[str]:\n",
    "    \"\"\"Return up to *k* PMIDs ranked by PubMed‚Äôs relevance score.\"\"\"\n",
    "    for attempt in range(1, 4):\n",
    "        try:\n",
    "            handle = Entrez.esearch(\n",
    "                db=\"pubmed\",\n",
    "                term=query,\n",
    "                retmax=k,\n",
    "                sort=\"relevance\",\n",
    "                retmode=\"xml\"\n",
    "            )\n",
    "            ids = Entrez.read(handle).get(\"IdList\", [])\n",
    "            time.sleep(SLEEP)             # respect 10 req/s limit when connected to API\n",
    "            return ids\n",
    "        except Exception as exc:\n",
    "            logging.warning(f\"E-search retry {attempt}: {exc!r}\")\n",
    "            time.sleep(2 ** attempt)      # exponential back-off\n",
    "    logging.error(f\"All retries failed for query: {query[:60]!r}\")\n",
    "    return []\n",
    "\n",
    "def pmid_to_url(pmid: str) -> str:\n",
    "    return f\"http://www.ncbi.nlm.nih.gov/pubmed/{pmid}\"\n",
    "\n",
    "# MAIN WORK \n",
    "\n",
    "qs = json.loads(TRAIN.read_text())[\"questions\"]\n",
    "print(f\"üóíÔ∏è  Loaded {len(qs)} questions from {TRAIN.name}\")\n",
    "\n",
    "predictions = []\n",
    "for q in tqdm(qs, unit=\"Q\"):\n",
    "    pmids = esearch_pmids(q[\"body\"])\n",
    "    predictions.append({\n",
    "        \"id\": q[\"id\"],\n",
    "        \"documents\": [pmid_to_url(p) for p in pmids][:MAX_DOCS],\n",
    "        \"snippets\": []                      # keep the field, even if empty becuase the test expects it\n",
    "    })\n",
    "\n",
    "OUT.write_text(json.dumps({\"questions\": predictions}, indent=2))\n",
    "print(f\"‚úÖ  Wrote Phase-A run file ‚Üí {OUT.resolve()}\")\n",
    "print(\"‚ö†Ô∏è  Check phaseA_esearch.log for warnings or API errors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Score:\n",
    "| Metric Index | Metric Name                          | Value                     |\n",
    "|--------------|--------------------------------------|---------------------------|\n",
    "| 1            | MAP (Mean Average Precision)         | 0.09842234475283342       |\n",
    "| 2            | GMAP (Geometric Mean Average Prec.)  | 0.1201860059504669        |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM 25 Ranking\n",
    "\n",
    "1. We are basically pooling for teh API and then fetching Title+Abstract and ranking using the BM 25 Ranking Algorithm(see below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{score}(q,d)= \\sum_{t\\in q} \\underbrace{\\text{IDF}(t)}_{\\text{rarer words get more weight}} \\cdot\n",
    "\\frac{f_{t,d}\\,(k_1+1)}{f_{t,d}+k_1\\bigl(1-b+b\\cdot\\frac{|d|}{\\mathrm{avgdl}}\\bigr)}\n",
    "$$\n",
    "\n",
    "* $f_{t,d}$‚ÄÉ= how many times term *t* appears in *d*  \n",
    "* $|d| / avgdl$‚ÄÖ‚ÄÉ= length-normalisation so long abstracts aren‚Äôt unfairly boosted  \n",
    "* **IDF(t)**‚ÄÉ‚ÄÉ‚ÄÉ= inverse-document-frequency: words that appear in *many* abstracts (‚Äúthe‚Äù, ‚Äúcell‚Äù) get near-zero weight, rare biomedical terms (‚Äútrastuzumab‚Äù) dominate  \n",
    "* $k_1,\\,b$‚ÄÉ= tuning constants (rank_bm25 defaults: 1.5, 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóíÔ∏è  Loaded 5389 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BM25 pipeline: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5389/5389 [1:58:50<00:00,  1.32s/Q]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ  BM25 submission written ‚Üí /Users/greinaldpappa/Downloads/BioASQ-training13b/bm25_phaseA_run.json\n",
      "‚ÑπÔ∏è  Baseline file remains at esearch_phaseA_run.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ROOT        = Path(\"/Users/greinaldpappa/Downloads/BioASQ-training13b\")\n",
    "TRAIN_FILE  = ROOT / \"training13b.json\"\n",
    "RUN_FILE    = ROOT / \"bm25_phaseA_run.json\"  \n",
    "CACHE_DIR   = ROOT / \"medline_cache\"\n",
    "CACHE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Get your API and put it HERE https://account.ncbi.nlm.nih.gov/settings/\n",
    "EMAIL   = \"pgreinald@gmail.com\"\n",
    "API_KEY = \"9666f51fccbd68a29320334f1d78ad257608\"\n",
    "\n",
    "# hyper-params you can play around with these to see if you get different results(Only play with candidates)\n",
    "CANDIDATES = 120    # eSearch pool\n",
    "MAX_DOCS   = 10     # BM25 output size. Limited according to https://www.bioasq.org/\n",
    "SLEEP      = 0.11   \n",
    "\n",
    "\n",
    "logging.basicConfig(filename=ROOT / \"bm25_phaseA.log\",\n",
    "                    level=logging.WARNING,\n",
    "                    format=\"%(asctime)s %(levelname)s %(message)s\")\n",
    "\n",
    "Entrez.email, Entrez.api_key = EMAIL, API_KEY\n",
    "\n",
    "# Nothing changed from before just a normal search like in google.\n",
    "def fetch_candidates(question: str, k: int = CANDIDATES) -> List[str]:\n",
    "    for attempt in range(1, 4):\n",
    "        try:\n",
    "            h = Entrez.esearch(db=\"pubmed\", term=question,\n",
    "                               sort=\"relevance\", retmax=k, retmode=\"xml\")\n",
    "            ids = Entrez.read(h).get(\"IdList\", [])\n",
    "            time.sleep(SLEEP)\n",
    "            return ids\n",
    "        except Exception as exc:\n",
    "            logging.warning(f\"eSearch retry {attempt}: {exc!r}\")\n",
    "            time.sleep(2 ** attempt)\n",
    "    logging.error(f\"eSearch failed for {question[:60]!r}\")\n",
    "    return []\n",
    "\n",
    "# Here the caching happens. The cache is a local file system cache where we save title and Abstract\n",
    "def _cache(pmid: str) -> Path:\n",
    "    return CACHE_DIR / f\"{pmid}.txt\"\n",
    "\n",
    "def get_abstracts(pmids: List[str]) -> Dict[str, str]:\n",
    "    texts, miss = {}, []\n",
    "    for p in pmids:\n",
    "        fp = _cache(p)\n",
    "        if fp.exists():\n",
    "            texts[p] = fp.read_text()\n",
    "        else:\n",
    "            miss.append(p)\n",
    "\n",
    "    for start in trange(0, len(miss), 200, leave=False, desc=\"efetch\"):\n",
    "        batch = miss[start:start+200]\n",
    "        if not batch: break\n",
    "        try:\n",
    "            h = Entrez.efetch(db=\"pubmed\", id=\",\".join(batch),\n",
    "                              rettype=\"medline\", retmode=\"text\")\n",
    "            for rec in Medline.parse(h):\n",
    "                txt = f\"{rec.get('TI','')} {rec.get('AB','')}\".strip()\n",
    "                _cache(rec[\"PMID\"]).write_text(txt)\n",
    "                texts[rec[\"PMID\"]] = txt\n",
    "        except Exception as exc:\n",
    "            logging.warning(f\"EFetch chunk failed: {exc!r}\")\n",
    "        time.sleep(SLEEP)\n",
    "\n",
    "    for p in pmids:\n",
    "        texts.setdefault(p, \"\")\n",
    "    return texts\n",
    "\n",
    "#  BM25 rerank \n",
    "def bm25_top10(question: str, pmids: List[str], texts: Dict[str, str]) -> List[str]:\n",
    "    usable = [p for p in pmids if texts[p].strip()]\n",
    "    if not usable:\n",
    "        return pmids[:MAX_DOCS]\n",
    "    corpus = [texts[p].lower().split() for p in usable]\n",
    "    bm25   = BM25Okapi(corpus)\n",
    "    q_tok  = question.lower().split()\n",
    "    scores = bm25.get_scores(q_tok)\n",
    "    ranked = [p for p, _ in sorted(zip(usable, scores), key=lambda x: -x[1])]\n",
    "    return ranked[:MAX_DOCS]\n",
    "\n",
    "# Picks the sentence in the abstract with the most word-overlap with the question and returns it as a BioASQ-formatted snippet\n",
    "_punct = str.maketrans(string.punctuation, \" \" * len(string.punctuation))\n",
    "def best_snippet(question: str, pmid: str, text: str):\n",
    "    if not text: return None\n",
    "    q_terms = set(question.lower().translate(_punct).split())\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    best = max(sentences,\n",
    "               key=lambda s: len(set(s.lower().split()) & q_terms),\n",
    "               default=\"\")\n",
    "    if not best: return None\n",
    "    return {\n",
    "        \"document\": pmid_to_url(pmid),\n",
    "        \"text\": best,\n",
    "        \"offsetInBeginSection\": 0,\n",
    "        \"offsetInEndSection\": len(best),\n",
    "        \"beginSection\": \"abstract\",\n",
    "        \"endSection\": \"abstract\"\n",
    "    }\n",
    "\n",
    "def pmid_to_url(pid: str) -> str:\n",
    "    return f\"http://www.ncbi.nlm.nih.gov/pubmed/{pid}\"\n",
    "\n",
    "# Main function to run the pipeline\n",
    "def main():\n",
    "    qs = json.loads(TRAIN_FILE.read_text())[\"questions\"]\n",
    "    print(f\"üóíÔ∏è  Loaded {len(qs)} questions\")\n",
    "\n",
    "    out_qs = []\n",
    "    for q in tqdm(qs, unit=\"Q\", desc=\"BM25 pipeline\"):\n",
    "        pmids      = fetch_candidates(q[\"body\"])\n",
    "        abstracts  = get_abstracts(pmids)\n",
    "        top_pmids  = bm25_top10(q[\"body\"], pmids, abstracts)\n",
    "        docs       = [pmid_to_url(p) for p in top_pmids]\n",
    "\n",
    "        snips = []\n",
    "        for p in top_pmids:\n",
    "            s = best_snippet(q[\"body\"], p, abstracts[p])\n",
    "            if s: snips.append(s)\n",
    "            if len(snips) == 10: break\n",
    "\n",
    "        out_qs.append({\"id\": q[\"id\"], \"documents\": docs, \"snippets\": snips})\n",
    "\n",
    "    RUN_FILE.write_text(json.dumps({\"questions\": out_qs}, indent=2))\n",
    "    print(f\"‚úÖ  BM25 submission written ‚Üí {RUN_FILE}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    print(\"‚ÑπÔ∏è  Baseline file remains at esearch_phaseA_run.json\")\n",
    "    print(\"‚ö†Ô∏è  Check bm25_phaseA.log for warnings or API errors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Score:\n",
    "\n",
    "| Metric Index | Metric Name                          | Value                     |\n",
    "|--------------|--------------------------------------|---------------------------|\n",
    "| 1            | MAP (Mean Average Precision)         | 0.10938912894285033       |\n",
    "| 2            | GMAP (Geometric Mean Average Prec.)  | 0.12504363569986954       |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31415847095936167"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1693 / 5389"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions with ‚â•1 hit: 1693 / 5389\n",
      "Median hit position  : 2\n"
     ]
    }
   ],
   "source": [
    "# This is a quick check to see if the documents are being fetched correctly.\n",
    "# Quick check. As I assumed the documents are not being fetched correctly in the first place where we have more than 1 hit only 0.31% of the documents\n",
    "# but I dont understand why becuase we are looking at enough documents to expect good rankings but seeing from baseline retrival we are most of the time\n",
    "# retriving irelevant information.\n",
    "\n",
    "GOLD = json.load(open(TRAIN_FILE))[\"questions\"]\n",
    "RUN  = json.load(open(RUN_FILE))[\"questions\"]\n",
    "\n",
    "hit_positions = []\n",
    "for g, r in zip(GOLD, RUN):\n",
    "    gold_set = set(g[\"documents\"])\n",
    "    ranked   = r[\"documents\"]\n",
    "    pos      = next((i+1 for i, d in enumerate(ranked) if d in gold_set), None)\n",
    "    if pos: hit_positions.append(pos)\n",
    "\n",
    "print(\"Questions with ‚â•1 hit:\", len(hit_positions), \"/\", len(GOLD))\n",
    "print(\"Median hit position  :\", statistics.median(hit_positions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase A Plus which requires Answers and not just documents anymore\n",
    "## See Steps below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## üîÑ Phase-A+ QA Pipeline: Step-by-Step\n",
    "\n",
    "### **Step 1: Classify the Question Type**\n",
    "- **Input**: Raw question text (e.g., `\"Is Hirschsprung disease a mendelian or a multifactorial disorder?\"`)\n",
    "- **Model**: BioBERT-based sequence classifier  \n",
    "  (`myctgh/biobert-question-type-bioasq13b`)\n",
    "- **Tokenizer**: `dmis-lab/biobert-base-cased-v1.1`\n",
    "- **Output**: One of the four types:\n",
    "  - `\"yesno\"`\n",
    "  - `\"factoid\"`\n",
    "  - `\"list\"`\n",
    "  - `\"summary\"`\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Select the Answering Strategy Based on Type**\n",
    "\n",
    "#### üîπ If `yesno`\n",
    "- **Model**: BioBERT binary classifier  \n",
    "  (`myctgh/biobert-yesno-bioasq13b`)\n",
    "- **Tokenizer**: Reuse BioBERT tokenizer\n",
    "- **Task**: Predict \"yes\" or \"no\" based on question and context\n",
    "\n",
    "---\n",
    "\n",
    "#### üîπ If `factoid` or `list`\n",
    "- **Model**: RoBERTa QA model  \n",
    "  (`deepset/roberta-base-squad2`)\n",
    "- **Tokenizer**: RoBERTa tokenizer\n",
    "- **Task**: Extract spans from the context as answers\n",
    "\n",
    "---\n",
    "\n",
    "#### üîπ If `summary`\n",
    "- **Model**: T5-based abstractive summarizer  \n",
    "  (`google/bioctrl-t5-base`)\n",
    "- **Tokenizer**: T5 tokenizer\n",
    "- **Task**: Generate a short natural-language summary from context\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: Save Answers**\n",
    "- Store both:\n",
    "  - `\"exact_answer\"`: from yes/no, span, or list models\n",
    "  - `\"ideal_answer\"`: generated summary\n",
    "- Save to: `answers_phaseA+.json`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phaseA_plus.\n",
    "# pip install transformers sentencepiece accelerate torch>=2.0\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Q-type classifier (4-way)\n",
    "# This here tokenizes the questions\n",
    "qtype_tok   = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
    "# This here is a model which is pretrained on the BioASQ dataset and fine-tuned to give the type of question\n",
    "qtype_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                 \"myctgh/biobert-question-type-bioasq13b\")  # fine-tuned model\n",
    "qtype_model.to(DEVICE).eval()  # Move model to device (GPU if available) and set to evaluation mode\n",
    "\n",
    "# Yes/No classifier (binary)\n",
    "# Reuses the same tokenizer as above (BioBERT-based)\n",
    "yn_tok   = qtype_tok\n",
    "# Loads a binary classifier (yes/no) fine-tuned on BioASQ yes/no questions\n",
    "yn_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "              \"myctgh/biobert-yesno-bioasq13b\").to(DEVICE).eval()\n",
    "\n",
    "# Extractive QA for factoid / list\n",
    "# Tokenizer for extractive QA using RoBERTa model fine-tuned on SQuAD2.0\n",
    "qa_tok   = AutoTokenizer.from_pretrained(\"deepset/roberta-base-squad2\")\n",
    "# Model for extractive QA (predicts answer spans in the context)\n",
    "qa_model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "              \"deepset/roberta-base-squad2\").to(DEVICE).eval()\n",
    "\n",
    "# HuggingFace pipeline to simplify question answering over a context\n",
    "# Only used if the question type is either factoid or list\n",
    "qa_pipe  = pipeline(\"question-answering\",\n",
    "                    model=qa_model, tokenizer=qa_tok,\n",
    "                    device=0 if DEVICE==\"cuda\" else -1)\n",
    "\n",
    "\n",
    "# Abstractive summariser for ‚Äúideal‚Äù answer\n",
    "# Tokenizer for T5 model fine-tuned on BioASQ summarization task\n",
    "sum_pipe = pipeline(\"summarization\",\n",
    "                    model=\"google/bioctrl-t5-base\",\n",
    "                    tokenizer=\"google/bioctrl-t5-base\",\n",
    "                    device=0 if DEVICE==\"cuda\" else -1,\n",
    "                    max_length=150)\n",
    "\n",
    "\n",
    "# Storing the question types in a list\n",
    "Q_TYPES = [\"yesno\", \"factoid\", \"list\", \"summary\"]   \n",
    "\n",
    "\n",
    "# This function takes a question string and returns its predicted type (\"yesno\", \"factoid\", \"list\", or \"summary\")\n",
    "def classify_question(q: str) -> str:\n",
    "    with torch.no_grad():  # Disable gradient tracking (inference mode, faster and uses less memory)\n",
    "        # Tokenize the input question and move tensors to the correct device (CPU or GPU) I dont know in the Jupiter Notebook it says that\n",
    "        # we are using the notebook is using GPU but when I go into it I just see mulitple CPUS(We can apply multithreding if no GPU since we have 40 CPUS).\n",
    "        inputs = qtype_tok(q, return_tensors=\"pt\").to(DEVICE)\n",
    "        \n",
    "        # Get the raw output logits from the classifier model\n",
    "        logits = qtype_model(**inputs).logits\n",
    "        \n",
    "        # Get the index of the highest logit (i.e., most probable class), convert to corresponding label. Basically we are\n",
    "        # getting out the index of the highest value in the logits and then we are using that index to get the type of question(Think of it as a vector where there are\n",
    "        # probabilities of each question type and we want the most probable one)\n",
    "        return Q_TYPES[int(logits.argmax())]\n",
    "\n",
    "\n",
    "# This function answers yes/no questions using the yes-no classifier model\n",
    "def yes_no_answer(q: str, ctx: str) -> str:\n",
    "    # Format input as \"question [SEP] context\" and tokenize it\n",
    "    inp = yn_tok(f\"{q} [SEP] {ctx}\", truncation=True,\n",
    "                 return_tensors=\"pt\").to(DEVICE)\n",
    "    \n",
    "    # Get logits and convert them to probabilities using softmax\n",
    "    # Output: [no_prob, yes_prob]\n",
    "    prob = yn_model(**inp).logits.softmax(-1).tolist()[0]\n",
    "    \n",
    "    # Return \"yes\" if yes_prob > no_prob, otherwise \"no\"\n",
    "    return \"yes\" if prob[1] > prob[0] else \"no\"\n",
    "\n",
    "# This function returns the top-k predicted spans from the context as answers to the question\n",
    "# Requires pulling exact words/phrases from the text, not generating them.\n",
    "def span_answers(q: str, ctx: str, top_k=5) -> list[str]:\n",
    "    # Use the extractive QA pipeline to get answers from the context\n",
    "    res = qa_pipe(question=q, context=ctx, top_k=top_k)\n",
    "    \n",
    "    # If multiple answers returned (list), extract each \"answer\" string\n",
    "    # Otherwise, return a single answer in a list\n",
    "    return [r[\"answer\"] for r in res] if isinstance(res, list) else [res[\"answer\"]]\n",
    "\n",
    "\n",
    "# This function processes a raw list of string answers (from extractive QA),\n",
    "# cleaning and formatting them for list-type questions.\n",
    "\n",
    "def post_process_list(raw: list[str]) -> list[list[str]]:\n",
    "    # Step 1: Clean each answer string\n",
    "    # - Remove all non-word characters except dash (-) and space\n",
    "    # - Strip leading/trailing whitespace\n",
    "    clean = [re.sub(r\"[^\\w\\- ]\", \"\", a).strip() for a in raw]\n",
    "    # Step 2: Deduplicate answers while preserving order (using dict.fromkeys)\n",
    "    # Step 3: Format each cleaned answer as a list of one item (as required by BioASQ format)\n",
    "    return [[a] for a in dict.fromkeys(clean) if a]\n",
    "\n",
    "\n",
    "# File Paths.\n",
    "DATA_DIR = Path(\"/Users/greinaldpappa/Downloads/BioASQ-training13b\")\n",
    "PHASEA   = DATA_DIR / \"bm25_phaseA_run.json\"\n",
    "OUTFILE  = DATA_DIR / \"answers_phaseA+.json\"\n",
    "\n",
    "# Load Phase-A BM25 results (predicted relevant snippets) from file\n",
    "phaseA = json.loads(PHASEA.read_text())[\"questions\"]\n",
    "\n",
    "answers_json = []  # This will store the final answers for all questions\n",
    "\n",
    "# Iterate over each question object in the Phase-A results\n",
    "for qobj in phaseA:\n",
    "    qid = qobj[\"id\"]  # Unique question ID\n",
    "    \n",
    "    # Retrieve the full question text (\"body\") by matching ID in the training file\n",
    "    query = next(q[\"body\"] for q in\n",
    "                 json.loads((DATA_DIR / \"training13b.json\").read_text())[\"questions\"]\n",
    "                 if q[\"id\"] == qid)\n",
    "\n",
    "    # Predict the type of the question: yesno, factoid, list, or summary\n",
    "    qtype = classify_question(query)\n",
    "\n",
    "    # Concatenate all retrieved snippets into a single context string (truncate to 3500 characters)\n",
    "    ctx = \" \".join(s[\"text\"] for s in qobj[\"snippets\"])[:3500]\n",
    "\n",
    "    # Choose the appropriate answering strategy based on question type\n",
    "    if qtype == \"yesno\":\n",
    "        # Use binary classifier to return \"yes\" or \"no\"\n",
    "        exact = yes_no_answer(query, ctx)\n",
    "    elif qtype in {\"factoid\", \"list\"}:\n",
    "        # Use extractive QA model to get answer spans\n",
    "        raw = span_answers(query, ctx, top_k=8)\n",
    "        # Clean and format answers (e.g., [[\"fever\"], [\"cough\"]])\n",
    "        exact = post_process_list(raw)\n",
    "    else:\n",
    "        # For summary questions, leave exact answer empty\n",
    "        exact = []\n",
    "\n",
    "    # Generate an ideal natural-language summary using the summarization pipeline\n",
    "    ideal = sum_pipe(ctx, min_length=30, max_length=120,\n",
    "                     do_sample=False)[0][\"summary_text\"]\n",
    "\n",
    "    # Append the answer object to the final output list\n",
    "    answers_json.append({\n",
    "        \"id\": qid,\n",
    "        \"type\": qtype,\n",
    "        \"exact_answer\": exact,\n",
    "        \"ideal_answer\": ideal.strip()\n",
    "    })\n",
    "    \n",
    "# dump submission\n",
    "OUTFILE.write_text(json.dumps({\"questions\": answers_json}, indent=2))\n",
    "print(\"‚úÖ  Phase-A* answers written ‚Üí\", OUTFILE)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
