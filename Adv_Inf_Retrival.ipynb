{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tabel\"></a>\n",
    "<b>Table of contents:</b>\n",
    "\n",
    "\n",
    "* [1. Import Libraries & Dataset](#import)\n",
    "\n",
    "[🏠 Table of Contents](#tabel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Bio\n",
      "  Downloading bio-1.7.1-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting biopython>=1.80 (from Bio)\n",
      "  Downloading biopython-1.85-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting gprofiler-official (from Bio)\n",
      "  Downloading gprofiler_official-1.0.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting mygene (from Bio)\n",
      "  Downloading mygene-3.2.2-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from Bio)\n",
      "  Using cached pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Collecting pooch (from Bio)\n",
      "  Downloading pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting requests (from Bio)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.12/site-packages (from Bio) (4.67.1)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (from biopython>=1.80->Bio) (2.2.5)\n",
      "Collecting biothings-client>=0.2.6 (from mygene->Bio)\n",
      "  Downloading biothings_client-0.4.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas->Bio) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->Bio)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->Bio)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in ./.venv/lib/python3.12/site-packages (from pooch->Bio) (4.3.7)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from pooch->Bio) (25.0)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->Bio)\n",
      "  Downloading charset_normalizer-3.4.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->Bio)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->Bio)\n",
      "  Downloading urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->Bio)\n",
      "  Downloading certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting httpx>=0.22.0 (from biothings-client>=0.2.6->mygene->Bio)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->Bio) (1.17.0)\n",
      "Collecting anyio (from httpx>=0.22.0->biothings-client>=0.2.6->mygene->Bio)\n",
      "  Downloading anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting httpcore==1.* (from httpx>=0.22.0->biothings-client>=0.2.6->mygene->Bio)\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx>=0.22.0->biothings-client>=0.2.6->mygene->Bio)\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting sniffio>=1.1 (from anyio->httpx>=0.22.0->biothings-client>=0.2.6->mygene->Bio)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting typing_extensions>=4.5 (from anyio->httpx>=0.22.0->biothings-client>=0.2.6->mygene->Bio)\n",
      "  Using cached typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Downloading bio-1.7.1-py3-none-any.whl (280 kB)\n",
      "Downloading biopython-1.85-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m578.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:02\u001b[0m\n",
      "\u001b[?25hDownloading gprofiler_official-1.0.0-py3-none-any.whl (9.3 kB)\n",
      "Downloading mygene-3.2.2-py2.py3-none-any.whl (5.4 kB)\n",
      "Using cached pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "Downloading pooch-1.8.2-py3-none-any.whl (64 kB)\n",
      "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading biothings_client-0.4.1-py3-none-any.whl (46 kB)\n",
      "Downloading certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
      "Downloading charset_normalizer-3.4.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (148 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "Installing collected packages: pytz, urllib3, tzdata, typing_extensions, sniffio, idna, h11, charset-normalizer, certifi, biopython, requests, pandas, httpcore, anyio, pooch, httpx, gprofiler-official, biothings-client, mygene, Bio\n",
      "Successfully installed Bio-1.7.1 anyio-4.9.0 biopython-1.85 biothings-client-0.4.1 certifi-2025.4.26 charset-normalizer-3.4.2 gprofiler-official-1.0.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 idna-3.10 mygene-3.2.2 pandas-2.2.3 pooch-1.8.2 pytz-2025.2 requests-2.32.3 sniffio-1.3.1 typing_extensions-4.13.2 tzdata-2025.2 urllib3-2.4.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install Bio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    ".phase-container {\n",
    "    display: flex;\n",
    "    flex-wrap: wrap;\n",
    "    justify-content: center;\n",
    "    gap: 15px;\n",
    "    padding: 20px;\n",
    "    font-family: 'Poppins', Roboto, sans-serif;\n",
    "}\n",
    "\n",
    ".phase {\n",
    "    width: 260px;\n",
    "    height: 70px;\n",
    "    line-height: 70px;\n",
    "    color: white;\n",
    "    font-weight: 600;\n",
    "    text-align: center;\n",
    "    border-radius: 6px;\n",
    "    position: relative;\n",
    "    font-size: 1.05em;\n",
    "    clip-path: polygon(0% 0%, 92% 0%, 100% 50%, 92% 100%, 0% 100%);\n",
    "    box-shadow: 2px 4px 10px rgba(0, 0, 0, 0.2);\n",
    "    opacity: 0.9;\n",
    "    transition: all 0.3s ease;\n",
    "    flex-shrink: 0;\n",
    "}\n",
    "\n",
    ".phase:hover {\n",
    "    transform: scale(1.05);\n",
    "    box-shadow: 4px 6px 15px rgba(0, 0, 0, 0.25);\n",
    "    opacity: 1;\n",
    "}\n",
    "\n",
    ".phase1 { background: #69B8F7; }\n",
    ".phase2 { background: #54A6F3; }\n",
    ".phase3 { background: #2B7BC1; }\n",
    ".phase4 { background: #1F557F; }\n",
    ".phase5 { background: #5C9D99; }\n",
    ".phase6 { background: #4E9B97; }\n",
    "\n",
    "@media (max-width: 768px) {\n",
    "    .phase {\n",
    "        width: 90%;\n",
    "    }\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div class=\"phase-container\">\n",
    "    <div class=\"phase phase1\">1. Create Baseline</div>\n",
    "    <div class=\"phase phase2\">2. Add BM25 (Retrieval Algorithm)</div>\n",
    "    <div class=\"phase phase3\">3. Add Classification of Questions Model</div>\n",
    "    <div class=\"phase phase4\">4. Add Model for Yes/No Questions</div>\n",
    "    <div class=\"phase phase5\">5. Add Model for Factoid, List Questions</div>\n",
    "    <div class=\"phase phase6\">6. Add Model for Summary Questions</div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Imports\n",
    "\n",
    "\n",
    "<a id=\"import\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library\n",
    "import json           \n",
    "import time           \n",
    "import logging        \n",
    "import string        \n",
    "import statistics\n",
    "import re             \n",
    "from pathlib import Path \n",
    "from typing import List, Dict\n",
    "\n",
    "# Bioinformatics Libraries\n",
    "from Bio import Entrez, Medline   # For accessing and parsing PubMed/NCBI data\n",
    "\n",
    "# Text Search / Ranking\n",
    "from rank_bm25 import BM25Okapi   # For BM25 ranking algorithm. Extension of the TF-IDF (Term Frequency-Inverse Document Frequency) model, taking into account term frequency saturation and document length to improve ranking accuracy. \n",
    "\n",
    "\n",
    "# NLP and Tokenization Tools\n",
    "import nltk                       # Natural Language Toolkit (tokenization, stopwords, etc.)\n",
    "\n",
    "# Progress Visualization\n",
    "from tqdm import tqdm, trange    \n",
    "\n",
    "# Machine Learning Libraries\n",
    "# import torch\n",
    "# from transformers import (\n",
    "#     AutoTokenizer, AutoModelForSequenceClassification,\n",
    "#     AutoModelForQuestionAnswering, pipeline\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nlogging.basicConfig(\\n    filename=\"phaseA_errors.log\",\\n    level=logging.WARNING,\\n    format=\"%(asctime)s %(levelname)s %(message)s\"\\n)\\n\\n\\nTRAIN = Path(\"/Users/greinaldpappa/Downloads/BioASQ-training13b/training13b.json\")\\nOUT   = Path(\"./api_phaseA_run.json\")\\n\\n\\ndata = json.loads(TRAIN.read_text())\\nqs   = data[\"questions\"]\\nprint(f\"🗒️  Loaded {len(qs)} questions.\")\\n\\n\\nsess = start_session()\\nprint(\"🗝️  API session:\", sess)\\n\\n\\npredictions = []\\nfor q in tqdm.tqdm(qs, unit=\"Q\"):\\n    qid, query = q[\"id\"], q[\"body\"]\\n    pmids = []\\n    for attempt in range(1, 4):       # up to 3 tries per question\\n        try:\\n            pmids = find_pubmed_citations(sess, query, k=1000)\\n            break                       # success → exit retry loop\\n        except Exception as exc:\\n            wait = 2 ** attempt         # exponential back-off: 2,4,8s\\n            logging.warning(\\n                f\"QID={qid} attempt {attempt} failed: {exc!r}; retrying in {wait}s\"\\n            )\\n            time.sleep(wait)\\n    else:\\n        # all retries failed → log and move on with empty list\\n        logging.error(f\"QID={qid} all retries failed; returning empty list\")\\n    predictions.append({\\n        \"query_id\": qid,\\n        \"documents\": pmids\\n    })\\n    time.sleep(0.1)                \\n\\n\\nOUT.write_text(json.dumps(predictions, indent=2))\\nprint(f\"✅  Wrote {len(predictions)} entries to {OUT}\")\\nprint(\"⚠️  If you saw errors, check phaseA_errors.log.\")'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Insanly Slow. Just know that till I state differently what we are doing here is nothing else but just a automatic search of the database\n",
    "# Without any aditional thing added. This is so to say like a random predictor and everything for phase A will be built on top of this.\n",
    "\"\"\"\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=\"phaseA_errors.log\",\n",
    "    level=logging.WARNING,\n",
    "    format=\"%(asctime)s %(levelname)s %(message)s\"\n",
    ")\n",
    "\n",
    "\n",
    "TRAIN = Path(\"/Users/greinaldpappa/Downloads/BioASQ-training13b/training13b.json\")\n",
    "OUT   = Path(\"./api_phaseA_run.json\")\n",
    "\n",
    "\n",
    "data = json.loads(TRAIN.read_text())\n",
    "qs   = data[\"questions\"]\n",
    "print(f\"🗒️  Loaded {len(qs)} questions.\")\n",
    "\n",
    "\n",
    "sess = start_session()\n",
    "print(\"🗝️  API session:\", sess)\n",
    "\n",
    "\n",
    "predictions = []\n",
    "for q in tqdm.tqdm(qs, unit=\"Q\"):\n",
    "    qid, query = q[\"id\"], q[\"body\"]\n",
    "    pmids = []\n",
    "    for attempt in range(1, 4):       # up to 3 tries per question\n",
    "        try:\n",
    "            pmids = find_pubmed_citations(sess, query, k=1000)\n",
    "            break                       # success → exit retry loop\n",
    "        except Exception as exc:\n",
    "            wait = 2 ** attempt         # exponential back-off: 2,4,8s\n",
    "            logging.warning(\n",
    "                f\"QID={qid} attempt {attempt} failed: {exc!r}; retrying in {wait}s\"\n",
    "            )\n",
    "            time.sleep(wait)\n",
    "    else:\n",
    "        # all retries failed → log and move on with empty list\n",
    "        logging.error(f\"QID={qid} all retries failed; returning empty list\")\n",
    "    predictions.append({\n",
    "        \"query_id\": qid,\n",
    "        \"documents\": pmids\n",
    "    })\n",
    "    time.sleep(0.1)                \n",
    "\n",
    "\n",
    "OUT.write_text(json.dumps(predictions, indent=2))\n",
    "print(f\"✅  Wrote {len(predictions)} entries to {OUT}\")\n",
    "print(\"⚠️  If you saw errors, check phaseA_errors.log.\")\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Information Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_KEY = \"2bb9ddd9-1828-419d-81df-bd89c4a97130\"\n",
    "BASE_URL = \"https://data.bioontology.org/search\"\n",
    "\n",
    "def get_mesh_synonyms(query, limit=5):\n",
    "    params = {\n",
    "        'q': query,\n",
    "        'ontologies': 'MESH',\n",
    "        'apikey': API_KEY,\n",
    "        'include': 'synonym,prefLabel',\n",
    "        'pagesize': limit\n",
    "    }\n",
    "\n",
    "    response = requests.get(BASE_URL, params=params)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        return []\n",
    "\n",
    "    data = response.json()\n",
    "    synonyms = set()\n",
    "\n",
    "    for result in data.get('collection', []):\n",
    "        pref_label = result.get('prefLabel', '')\n",
    "        synonyms.add(pref_label.lower())\n",
    "        for syn in result.get('synonym', []):\n",
    "            synonyms.add(syn.lower())\n",
    "\n",
    "    synonyms.discard(query.lower())\n",
    "    return list(synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_query_with_mesh(query):\n",
    "    mesh_synonyms = get_mesh_synonyms(query)\n",
    "    terms = [f'\"{query}\"[Title/Abstract]']\n",
    "    terms += [f'\"{syn}\"[Title/Abstract]' for syn in mesh_synonyms]\n",
    "    terms += [f'\"{syn}\"[MeSH Terms]' for syn in mesh_synonyms]\n",
    "    expanded_query = ' OR '.join(terms)\n",
    "    return expanded_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean(query):\n",
    "#     query = query.translate(str.maketrans('', '', string.punctuation))\n",
    "#     return \" \".join(query.strip().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/arjol/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗒️  Loaded 5389 questions from training13b.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5389/5389 [3:16:48<00:00,  2.19s/Q]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅  Wrote Phase-A run file → /home/arjol/Documents/dev/SS25AIR_Group14/data/esearch_phaseA_run.json\n",
      "⚠️  Check phaseA_esearch.log for warnings or API errors.\n",
      "Questions with ≥1 hit: 4554 / 5389\n",
      "Median hit position  : 107.0\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from typing import List\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "\n",
    "TRAIN = Path(\"/home/arjol/Documents/dev/SS25AIR_Group14/data/training13b.json\")\n",
    "OUT   = Path(\"/home/arjol/Documents/dev/SS25AIR_Group14/data/esearch_phaseA_run.json\")\n",
    "\n",
    "# Get your API and put it HERE https://account.ncbi.nlm.nih.gov/settings/\n",
    "EMAIL   = \"apanci2000@gmail.com\"        \n",
    "API_KEY = \"a417cd398ae9ba5622989a1f8ef153750f08\"       \n",
    "\n",
    "MAX_DOCS = 10000       # list size required by BioASQ Phase-A\n",
    "RETMX    = 10000      # retrieve more, then truncate\n",
    "SLEEP    = 0.11      # 10 requests / sec with API key\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=\"/home/arjol/Documents/dev/SS25AIR_Group14/data/phaseA_esearch.log\",\n",
    "    level=logging.WARNING,\n",
    "    format=\"%(asctime)s %(levelname)s %(message)s\"\n",
    ")\n",
    "\n",
    "Entrez.email   = EMAIL\n",
    "Entrez.api_key = API_KEY\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def clean(query: str) -> (str, List[str]):\n",
    "    s = re.sub(r'[^a-z0-9\\s]', ' ', query.lower())\n",
    "    tokens = [t for t in word_tokenize(s) if len(t) >= 3]\n",
    "    phrase = \" \".join(tokens)\n",
    "    return phrase, tokens\n",
    "\n",
    "def esearch_pmids(query: str, k: int = RETMX) -> List[str]:\n",
    "    phrase, tokens = clean(query)\n",
    "    if not tokens:\n",
    "        return []\n",
    "    parts = []\n",
    "    # full‑phrase lookups\n",
    "    parts.append(f'\"{phrase}\"[Title/Abstract]')\n",
    "    parts.append(f'\"{phrase}\"[MeSH Terms]')\n",
    "    # individual‑token lookups\n",
    "    parts += [f'{t}[Title/Abstract]' for t in tokens]\n",
    "    parts += [f'{t}[MeSH Terms]' for t in tokens]\n",
    "    term = f\"({' OR '.join(parts)}) AND hasabstract[text]\"\n",
    "    for attempt in range(1, 4):\n",
    "        try:\n",
    "            handle = Entrez.esearch(\n",
    "                db=\"pubmed\",\n",
    "                term=term,\n",
    "                retmax=k,\n",
    "                sort=\"relevance\",\n",
    "                retmode=\"xml\"\n",
    "            )\n",
    "            ids = Entrez.read(handle).get(\"IdList\", [])\n",
    "            time.sleep(SLEEP)\n",
    "            return ids\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"eSearch retry {attempt}: {e!r}\")\n",
    "            time.sleep(2 ** attempt)\n",
    "    logging.error(f\"All eSearch retries failed for {query!r}\")\n",
    "    return []\n",
    "\n",
    "def pmid_to_url(pmid: str) -> str:\n",
    "    return f\"http://www.ncbi.nlm.nih.gov/pubmed/{pmid}\"\n",
    "\n",
    "# MAIN WORK \n",
    "\n",
    "qs = json.loads(TRAIN.read_text())[\"questions\"]\n",
    "print(f\"🗒️  Loaded {len(qs)} questions from {TRAIN.name}\")\n",
    "\n",
    "predictions = []\n",
    "for q in tqdm(qs, unit=\"Q\"):\n",
    "    pmids = esearch_pmids(q[\"body\"])\n",
    "    predictions.append({\n",
    "        \"id\": q[\"id\"],\n",
    "        \"documents\": [pmid_to_url(p) for p in pmids][:MAX_DOCS],\n",
    "        \"snippets\": []                      # keep the field, even if empty becuase the test expects it\n",
    "    })\n",
    "    count += 1\n",
    "\n",
    "OUT.write_text(json.dumps({\"questions\": predictions}, indent=2))    \n",
    "print(f\"✅  Wrote Phase-A run file → {OUT.resolve()}\")\n",
    "print(\"⚠️  Check phaseA_esearch.log for warnings or API errors.\")\n",
    "\n",
    "# This is a quick check to see if the documents are being fetched correctly.\n",
    "# Quick check. As I assumed the documents are not being fetched correctly in the first place where we have more than 1 hit only 0.31% of the documents\n",
    "# but I dont understand why becuase we are looking at enough documents to expect good rankings but seeing from baseline retrival we are most of the time\n",
    "# retriving irelevant information.\n",
    "TRAIN_FILE  = \"/home/arjol/Documents/dev/SS25AIR_Group14/data/training13b.json\"\n",
    "RUN_FILE  = \"/home/arjol/Documents/dev/SS25AIR_Group14/data/esearch_phaseA_run.json\"\n",
    "\n",
    "GOLD = json.load(open(TRAIN_FILE))[\"questions\"]\n",
    "RUN  = json.load(open(RUN_FILE))[\"questions\"]\n",
    "\n",
    "hit_positions = []\n",
    "for g, r in zip(GOLD, RUN):\n",
    "    gold_set = set(g[\"documents\"])\n",
    "    ranked   = r[\"documents\"]\n",
    "    pos      = next((i+1 for i, d in enumerate(ranked) if d in gold_set), None)\n",
    "    if pos: hit_positions.append(pos)\n",
    "\n",
    "print(\"Questions with ≥1 hit:\", len(hit_positions), \"/\", len(GOLD))\n",
    "print(\"Median hit position  :\", statistics.median(hit_positions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Score:\n",
    "| Metric Index | Metric Name                          | Value                     |\n",
    "|--------------|--------------------------------------|---------------------------|\n",
    "| 1            | MAP (Mean Average Precision)         | 0.09842234475283342       |\n",
    "| 2            | GMAP (Geometric Mean Average Prec.)  | 0.1201860059504669        |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM 25 Ranking\n",
    "\n",
    "1. We are basically pooling for teh API and then fetching Title+Abstract and ranking using the BM 25 Ranking Algorithm(see below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{score}(q,d)= \\sum_{t\\in q} \\underbrace{\\text{IDF}(t)}_{\\text{rarer words get more weight}} \\cdot\n",
    "\\frac{f_{t,d}\\,(k_1+1)}{f_{t,d}+k_1\\bigl(1-b+b\\cdot\\frac{|d|}{\\mathrm{avgdl}}\\bigr)}\n",
    "$$\n",
    "\n",
    "* $f_{t,d}$ = how many times term *t* appears in *d*  \n",
    "* $|d| / avgdl$  = length-normalisation so long abstracts aren’t unfairly boosted  \n",
    "* **IDF(t)**   = inverse-document-frequency: words that appear in *many* abstracts (“the”, “cell”) get near-zero weight, rare biomedical terms (“trastuzumab”) dominate  \n",
    "* $k_1,\\,b$ = tuning constants (rank_bm25 defaults: 1.5, 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average recall @ pool of 10 000: 0.632\n",
      "Median  recall @ pool of 10 000: 0.750\n",
      "\n",
      "Example per-question recall (first 5 questions):\n",
      " QID=55031181e9bde69634000014 | gold= 9 | hits= 1 | recall=0.111\n",
      " QID=55046d5ff8aee20f27000007 | gold=16 | hits=13 | recall=0.812\n",
      " QID=54e25eaaae9738404b000017 | gold=10 | hits= 8 | recall=0.800\n",
      " QID=535d292a9a4572de6f000003 | gold= 6 | hits= 5 | recall=0.833\n",
      " QID=55262a9787ecba3764000009 | gold=10 | hits= 8 | recall=0.800\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean, median\n",
    "\n",
    "TRAIN_FILE = Path(\"/home/arjol/Documents/dev/SS25AIR_Group14/data/training13b.json\")\n",
    "RUN_FILE   = Path(\"/home/arjol/Documents/dev/SS25AIR_Group14/data/esearch_phaseA_run.json\")\n",
    "\n",
    "def load_questions(path):\n",
    "    data = json.load(open(path))\n",
    "    return {q[\"id\"]: q[\"documents\"] for q in data[\"questions\"]}\n",
    "\n",
    "def compute_pool_recall(gold, run, pool_size=10000):\n",
    "    recalls = []\n",
    "    details = {}\n",
    "    for qid, gold_docs in gold.items():\n",
    "        gold_set = set(gold_docs)\n",
    "        retrieved = run.get(qid, [])[:pool_size]\n",
    "        hits = sum(1 for doc in retrieved if doc in gold_set)\n",
    "        gold_count = len(gold_set)\n",
    "        recall = hits / gold_count if gold_count else 0.0\n",
    "        recalls.append(recall)\n",
    "        details[qid] = (gold_count, hits, recall)\n",
    "    return recalls, details\n",
    "\n",
    "\n",
    "gold = load_questions(TRAIN_FILE)\n",
    "run  = load_questions(RUN_FILE)\n",
    "\n",
    "recalls, details = compute_pool_recall(gold, run, pool_size=10000)\n",
    "\n",
    "# Overall metrics\n",
    "avg_recall = mean(recalls)\n",
    "med_recall = median(recalls)\n",
    "print(f\"Average recall @ pool of 10 000: {avg_recall:.3f}\")\n",
    "print(f\"Median  recall @ pool of 10 000: {med_recall:.3f}\")\n",
    "\n",
    "# Optional: print per-question breakdown for the first few\n",
    "print(\"\\nExample per-question recall (first 5 questions):\")\n",
    "for qid in list(details)[:5]:\n",
    "    gold_count, hits, recall = details[qid]\n",
    "    print(f\" QID={qid:10s} | gold={gold_count:2d} | hits={hits:2d} | recall={recall:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗒️  Loaded 5389 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BM25 pipeline: 100%|██████████| 5389/5389 [1:51:44<00:00,  1.24s/Q]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅  BM25 submission written → /home/arjol/Documents/dev/SS25AIR_Group14/data/esearch_phaseA_BM25_run.json\n",
      "ℹ️  Baseline file remains at esearch_phaseA_run.json\n",
      "⚠️  Check bm25_phaseA.log for warnings or API errors.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ROOT        = Path(\"/Users/greinaldpappa/Downloads/BioASQ-training13b\")\n",
    "TRAIN_FILE = Path(\"/home/arjol/Documents/dev/SS25AIR_Group14/data/training13b.json\")\n",
    "OUT   = Path(\"/home/arjol/Documents/dev/SS25AIR_Group14/data/esearch_phaseA_BM25_run.json\")  \n",
    "CACHE_DIR   = Path(\"medline_cache\")\n",
    "CACHE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Get your API and put it HERE https://account.ncbi.nlm.nih.gov/settings/\n",
    "EMAIL   = \"pgreinald@gmail.com\"\n",
    "API_KEY = \"9666f51fccbd68a29320334f1d78ad257608\"\n",
    "\n",
    "# hyper-params you can play around with these to see if you get different results(Only play with candidates)\n",
    "CANDIDATES = 200    # eSearch pool\n",
    "MAX_DOCS   = 10     # BM25 output size. Limited according to https://www.bioasq.org/\n",
    "SLEEP      = 0.11   \n",
    "\n",
    "\n",
    "logging.basicConfig(filename=\"bm25_phaseA.log\",\n",
    "                    level=logging.WARNING,\n",
    "                    format=\"%(asctime)s %(levelname)s %(message)s\")\n",
    "\n",
    "Entrez.email, Entrez.api_key = EMAIL, API_KEY\n",
    "def clean_tokens(query: str) -> (str, List[str]):\n",
    "    s = re.sub(r'[^a-z0-9\\s]', ' ', query.lower())\n",
    "    tokens = [t for t in word_tokenize(s) if len(t) >= 3]\n",
    "    return \" \".join(tokens), tokens\n",
    "\n",
    "def fetch_candidates(question: str, k: int = CANDIDATES) -> List[str]:\n",
    "    phrase, tokens = clean_tokens(question)\n",
    "    if not tokens:\n",
    "        return []\n",
    "    parts = [f'\"{phrase}\"[Title/Abstract]', f'\"{phrase}\"[MeSH Terms]']\n",
    "    parts += [f'{t}[Title/Abstract]' for t in tokens]\n",
    "    parts += [f'{t}[MeSH Terms]' for t in tokens]\n",
    "    term = f\"({' OR '.join(parts)}) AND hasabstract[text]\"\n",
    "    for attempt in range(1, 4):\n",
    "        try:\n",
    "            handle = Entrez.esearch(\n",
    "                db=\"pubmed\",\n",
    "                term=term,\n",
    "                retmax=k,\n",
    "                sort=\"relevance\",\n",
    "                retmode=\"xml\"\n",
    "            )\n",
    "            ids = Entrez.read(handle).get(\"IdList\", [])\n",
    "            time.sleep(SLEEP)\n",
    "            return ids\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"eSearch retry {attempt}: {e!r}\")\n",
    "            time.sleep(2 ** attempt)\n",
    "    logging.error(f\"All eSearch retries failed for {question!r}\")\n",
    "    return []\n",
    "\n",
    "# Here the caching happens. The cache is a local file system cache where we save title and Abstract\n",
    "def _cache(pmid: str) -> Path:\n",
    "    return CACHE_DIR / f\"{pmid}.txt\"\n",
    "\n",
    "def get_abstracts(pmids: List[str]) -> Dict[str, str]:\n",
    "    texts, miss = {}, []\n",
    "    for p in pmids:\n",
    "        fp = _cache(p)\n",
    "        if fp.exists():\n",
    "            texts[p] = fp.read_text()\n",
    "        else:\n",
    "            miss.append(p)\n",
    "\n",
    "    for start in trange(0, len(miss), 200, leave=False, desc=\"efetch\"):\n",
    "        batch = miss[start:start+200]\n",
    "        if not batch: break\n",
    "        try:\n",
    "            h = Entrez.efetch(db=\"pubmed\", id=\",\".join(batch),\n",
    "                              rettype=\"medline\", retmode=\"text\")\n",
    "            for rec in Medline.parse(h):\n",
    "                txt = f\"{rec.get('TI','')} {rec.get('AB','')}\".strip()\n",
    "                _cache(rec[\"PMID\"]).write_text(txt)\n",
    "                texts[rec[\"PMID\"]] = txt\n",
    "        except Exception as exc:\n",
    "            logging.warning(f\"EFetch chunk failed: {exc!r}\")\n",
    "        time.sleep(SLEEP)\n",
    "\n",
    "    for p in pmids:\n",
    "        texts.setdefault(p, \"\")\n",
    "    return texts\n",
    "\n",
    "#  BM25 rerank \n",
    "def bm25_top10(question: str, pmids: List[str], texts: Dict[str, str]) -> List[str]:\n",
    "    usable = [p for p in pmids if texts[p].strip()]\n",
    "    if not usable:\n",
    "        return pmids[:MAX_DOCS]\n",
    "    corpus = [texts[p].lower().split() for p in usable]\n",
    "    bm25   = BM25Okapi(corpus)\n",
    "    q_tok  = question.lower().split()\n",
    "    scores = bm25.get_scores(q_tok)\n",
    "    ranked = [p for p, _ in sorted(zip(usable, scores), key=lambda x: -x[1])]\n",
    "    return ranked[:MAX_DOCS]\n",
    "\n",
    "# Picks the sentence in the abstract with the most word-overlap with the question and returns it as a BioASQ-formatted snippet\n",
    "_punct = str.maketrans(string.punctuation, \" \" * len(string.punctuation))\n",
    "def best_snippet(question: str, pmid: str, text: str):\n",
    "    if not text: return None\n",
    "    q_terms = set(question.lower().translate(_punct).split())\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    best = max(sentences,\n",
    "               key=lambda s: len(set(s.lower().split()) & q_terms),\n",
    "               default=\"\")\n",
    "    if not best: return None\n",
    "    return {\n",
    "        \"document\": pmid_to_url(pmid),\n",
    "        \"text\": best,\n",
    "        \"offsetInBeginSection\": 0,\n",
    "        \"offsetInEndSection\": len(best),\n",
    "        \"beginSection\": \"abstract\",\n",
    "        \"endSection\": \"abstract\"\n",
    "    }\n",
    "\n",
    "def pmid_to_url(pid: str) -> str:\n",
    "    return f\"http://www.ncbi.nlm.nih.gov/pubmed/{pid}\"\n",
    "\n",
    "# Main function to run the pipeline\n",
    "def main():\n",
    "    qs = json.loads(TRAIN_FILE.read_text())[\"questions\"]\n",
    "    print(f\"🗒️  Loaded {len(qs)} questions\")\n",
    "\n",
    "    out_qs = []\n",
    "    for q in tqdm(qs, unit=\"Q\", desc=\"BM25 pipeline\"):\n",
    "        pmids      = fetch_candidates(q[\"body\"])\n",
    "        abstracts  = get_abstracts(pmids)\n",
    "        top_pmids  = bm25_top10(q[\"body\"], pmids, abstracts)\n",
    "        docs       = [pmid_to_url(p) for p in top_pmids]\n",
    "\n",
    "        snips = []\n",
    "        for p in top_pmids:\n",
    "            s = best_snippet(q[\"body\"], p, abstracts[p])\n",
    "            if s: snips.append(s)\n",
    "            if len(snips) == 10: break\n",
    "\n",
    "        out_qs.append({\"id\": q[\"id\"], \"documents\": docs, \"snippets\": snips})\n",
    "\n",
    "    OUT.write_text(json.dumps({\"questions\": out_qs}, indent=2))\n",
    "    print(f\"✅  BM25 submission written → {OUT}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    print(\"ℹ️  Baseline file remains at esearch_phaseA_run.json\")\n",
    "    print(\"⚠️  Check bm25_phaseA.log for warnings or API errors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "java - Xmx2G -cp \\\n",
    " /home/arjol/Documents/dev/Evaluation-Measures/flat/BioASQEvaluation/dist/BioASQEvaluation.jar \\\n",
    " evaluation.EvaluatorTask1b -phaseA \\\n",
    " /home/arjol/Documents/dev/SS25AIR_Group14/data/training13b.json \\\n",
    " /home/arjol/Documents/dev/SS25AIR_Group14/data/esearch_phaseA_BM25_run.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Score:\n",
    "\n",
    "| Metric Index | Metric Name                          | Value                     |\n",
    "|--------------|--------------------------------------|---------------------------|\n",
    "| 1            | MAP (Mean Average Precision)         | 0.10938912894285033       |\n",
    "| 2            | GMAP (Geometric Mean Average Prec.)  | 0.12504363569986954       |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31415847095936167"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1693 / 5389"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions with ≥1 hit: 1731 / 5389\n",
      "Median hit position  : 2\n"
     ]
    }
   ],
   "source": [
    "# This is a quick check to see if the documents are being fetched correctly.\n",
    "# Quick check. As I assumed the documents are not being fetched correctly in the first place where we have more than 1 hit only 0.31% of the documents\n",
    "# but I dont understand why becuase we are looking at enough documents to expect good rankings but seeing from baseline retrival we are most of the time\n",
    "# retriving irelevant information.\n",
    "TRAIN_FILE  = \"/home/arjol/Documents/dev/SS25AIR_Group14/data/training13b.json\"\n",
    "RUN_FILE  = \"/home/arjol/Documents/dev/SS25AIR_Group14/data/esearch_phaseA_BM25_run.json\"\n",
    "\n",
    "GOLD = json.load(open(TRAIN_FILE))[\"questions\"]\n",
    "RUN  = json.load(open(RUN_FILE))[\"questions\"]\n",
    "\n",
    "hit_positions = []\n",
    "for g, r in zip(GOLD, RUN):\n",
    "    gold_set = set(g[\"documents\"])\n",
    "    ranked   = r[\"documents\"]\n",
    "    pos      = next((i+1 for i, d in enumerate(ranked) if d in gold_set), None)\n",
    "    if pos: hit_positions.append(pos)\n",
    "\n",
    "print(\"Questions with ≥1 hit:\", len(hit_positions), \"/\", len(GOLD))\n",
    "print(\"Median hit position  :\", statistics.median(hit_positions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase A Plus which requires Answers and not just documents anymore\n",
    "## See Steps below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 🔄 Phase-A+ QA Pipeline: Step-by-Step\n",
    "\n",
    "### **Step 1: Classify the Question Type**\n",
    "- **Input**: Raw question text (e.g., `\"Is Hirschsprung disease a mendelian or a multifactorial disorder?\"`)\n",
    "- **Model**: BioBERT-based sequence classifier  \n",
    "  (`myctgh/biobert-question-type-bioasq13b`)\n",
    "- **Tokenizer**: `dmis-lab/biobert-base-cased-v1.1`\n",
    "- **Output**: One of the four types:\n",
    "  - `\"yesno\"`\n",
    "  - `\"factoid\"`\n",
    "  - `\"list\"`\n",
    "  - `\"summary\"`\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Select the Answering Strategy Based on Type**\n",
    "\n",
    "#### 🔹 If `yesno`\n",
    "- **Model**: BioBERT binary classifier  \n",
    "  (`myctgh/biobert-yesno-bioasq13b`)\n",
    "- **Tokenizer**: Reuse BioBERT tokenizer\n",
    "- **Task**: Predict \"yes\" or \"no\" based on question and context\n",
    "\n",
    "---\n",
    "\n",
    "#### 🔹 If `factoid` or `list`\n",
    "- **Model**: RoBERTa QA model  \n",
    "  (`deepset/roberta-base-squad2`)\n",
    "- **Tokenizer**: RoBERTa tokenizer\n",
    "- **Task**: Extract spans from the context as answers\n",
    "\n",
    "---\n",
    "\n",
    "#### 🔹 If `summary`\n",
    "- **Model**: T5-based abstractive summarizer  \n",
    "  (`google/bioctrl-t5-base`)\n",
    "- **Tokenizer**: T5 tokenizer\n",
    "- **Task**: Generate a short natural-language summary from context\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: Save Answers**\n",
    "- Store both:\n",
    "  - `\"exact_answer\"`: from yes/no, span, or list models\n",
    "  - `\"ideal_answer\"`: generated summary\n",
    "- Save to: `answers_phaseA+.json`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phaseA_plus.\n",
    "# pip install transformers sentencepiece accelerate torch>=2.0\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Q-type classifier (4-way)\n",
    "# This here tokenizes the questions\n",
    "qtype_tok   = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
    "# This here is a model which is pretrained on the BioASQ dataset and fine-tuned to give the type of question\n",
    "qtype_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                 \"myctgh/biobert-question-type-bioasq13b\")  # fine-tuned model\n",
    "qtype_model.to(DEVICE).eval()  # Move model to device (GPU if available) and set to evaluation mode\n",
    "\n",
    "# Yes/No classifier (binary)\n",
    "# Reuses the same tokenizer as above (BioBERT-based)\n",
    "yn_tok   = qtype_tok\n",
    "# Loads a binary classifier (yes/no) fine-tuned on BioASQ yes/no questions\n",
    "yn_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "              \"myctgh/biobert-yesno-bioasq13b\").to(DEVICE).eval()\n",
    "\n",
    "# Extractive QA for factoid / list\n",
    "# Tokenizer for extractive QA using RoBERTa model fine-tuned on SQuAD2.0\n",
    "qa_tok   = AutoTokenizer.from_pretrained(\"deepset/roberta-base-squad2\")\n",
    "# Model for extractive QA (predicts answer spans in the context)\n",
    "qa_model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "              \"deepset/roberta-base-squad2\").to(DEVICE).eval()\n",
    "\n",
    "# HuggingFace pipeline to simplify question answering over a context\n",
    "# Only used if the question type is either factoid or list\n",
    "qa_pipe  = pipeline(\"question-answering\",\n",
    "                    model=qa_model, tokenizer=qa_tok,\n",
    "                    device=0 if DEVICE==\"cuda\" else -1)\n",
    "\n",
    "\n",
    "# Abstractive summariser for “ideal” answer\n",
    "# Tokenizer for T5 model fine-tuned on BioASQ summarization task\n",
    "sum_pipe = pipeline(\"summarization\",\n",
    "                    model=\"google/bioctrl-t5-base\",\n",
    "                    tokenizer=\"google/bioctrl-t5-base\",\n",
    "                    device=0 if DEVICE==\"cuda\" else -1,\n",
    "                    max_length=150)\n",
    "\n",
    "\n",
    "# Storing the question types in a list\n",
    "Q_TYPES = [\"yesno\", \"factoid\", \"list\", \"summary\"]   \n",
    "\n",
    "\n",
    "# This function takes a question string and returns its predicted type (\"yesno\", \"factoid\", \"list\", or \"summary\")\n",
    "def classify_question(q: str) -> str:\n",
    "    with torch.no_grad():  # Disable gradient tracking (inference mode, faster and uses less memory)\n",
    "        # Tokenize the input question and move tensors to the correct device (CPU or GPU) I dont know in the Jupiter Notebook it says that\n",
    "        # we are using the notebook is using GPU but when I go into it I just see mulitple CPUS(We can apply multithreding if no GPU since we have 40 CPUS).\n",
    "        inputs = qtype_tok(q, return_tensors=\"pt\").to(DEVICE)\n",
    "        \n",
    "        # Get the raw output logits from the classifier model\n",
    "        logits = qtype_model(**inputs).logits\n",
    "        \n",
    "        # Get the index of the highest logit (i.e., most probable class), convert to corresponding label. Basically we are\n",
    "        # getting out the index of the highest value in the logits and then we are using that index to get the type of question(Think of it as a vector where there are\n",
    "        # probabilities of each question type and we want the most probable one)\n",
    "        return Q_TYPES[int(logits.argmax())]\n",
    "\n",
    "\n",
    "# This function answers yes/no questions using the yes-no classifier model\n",
    "def yes_no_answer(q: str, ctx: str) -> str:\n",
    "    # Format input as \"question [SEP] context\" and tokenize it\n",
    "    inp = yn_tok(f\"{q} [SEP] {ctx}\", truncation=True,\n",
    "                 return_tensors=\"pt\").to(DEVICE)\n",
    "    \n",
    "    # Get logits and convert them to probabilities using softmax\n",
    "    # Output: [no_prob, yes_prob]\n",
    "    prob = yn_model(**inp).logits.softmax(-1).tolist()[0]\n",
    "    \n",
    "    # Return \"yes\" if yes_prob > no_prob, otherwise \"no\"\n",
    "    return \"yes\" if prob[1] > prob[0] else \"no\"\n",
    "\n",
    "# This function returns the top-k predicted spans from the context as answers to the question\n",
    "# Requires pulling exact words/phrases from the text, not generating them.\n",
    "def span_answers(q: str, ctx: str, top_k=5) -> list[str]:\n",
    "    # Use the extractive QA pipeline to get answers from the context\n",
    "    res = qa_pipe(question=q, context=ctx, top_k=top_k)\n",
    "    \n",
    "    # If multiple answers returned (list), extract each \"answer\" string\n",
    "    # Otherwise, return a single answer in a list\n",
    "    return [r[\"answer\"] for r in res] if isinstance(res, list) else [res[\"answer\"]]\n",
    "\n",
    "\n",
    "# This function processes a raw list of string answers (from extractive QA),\n",
    "# cleaning and formatting them for list-type questions.\n",
    "\n",
    "def post_process_list(raw: list[str]) -> list[list[str]]:\n",
    "    # Step 1: Clean each answer string\n",
    "    # - Remove all non-word characters except dash (-) and space\n",
    "    # - Strip leading/trailing whitespace\n",
    "    clean = [re.sub(r\"[^\\w\\- ]\", \"\", a).strip() for a in raw]\n",
    "    # Step 2: Deduplicate answers while preserving order (using dict.fromkeys)\n",
    "    # Step 3: Format each cleaned answer as a list of one item (as required by BioASQ format)\n",
    "    return [[a] for a in dict.fromkeys(clean) if a]\n",
    "\n",
    "\n",
    "# File Paths.\n",
    "DATA_DIR = Path(\"/Users/greinaldpappa/Downloads/BioASQ-training13b\")\n",
    "PHASEA   = DATA_DIR / \"bm25_phaseA_run.json\"\n",
    "OUTFILE  = DATA_DIR / \"answers_phaseA+.json\"\n",
    "\n",
    "# Load Phase-A BM25 results (predicted relevant snippets) from file\n",
    "phaseA = json.loads(PHASEA.read_text())[\"questions\"]\n",
    "\n",
    "answers_json = []  # This will store the final answers for all questions\n",
    "\n",
    "# Iterate over each question object in the Phase-A results\n",
    "for qobj in phaseA:\n",
    "    qid = qobj[\"id\"]  # Unique question ID\n",
    "    \n",
    "    # Retrieve the full question text (\"body\") by matching ID in the training file\n",
    "    query = next(q[\"body\"] for q in\n",
    "                 json.loads((DATA_DIR / \"training13b.json\").read_text())[\"questions\"]\n",
    "                 if q[\"id\"] == qid)\n",
    "\n",
    "    # Predict the type of the question: yesno, factoid, list, or summary\n",
    "    qtype = classify_question(query)\n",
    "\n",
    "    # Concatenate all retrieved snippets into a single context string (truncate to 3500 characters)\n",
    "    ctx = \" \".join(s[\"text\"] for s in qobj[\"snippets\"])[:3500]\n",
    "\n",
    "    # Choose the appropriate answering strategy based on question type\n",
    "    if qtype == \"yesno\":\n",
    "        # Use binary classifier to return \"yes\" or \"no\"\n",
    "        exact = yes_no_answer(query, ctx)\n",
    "    elif qtype in {\"factoid\", \"list\"}:\n",
    "        # Use extractive QA model to get answer spans\n",
    "        raw = span_answers(query, ctx, top_k=8)\n",
    "        # Clean and format answers (e.g., [[\"fever\"], [\"cough\"]])\n",
    "        exact = post_process_list(raw)\n",
    "    else:\n",
    "        # For summary questions, leave exact answer empty\n",
    "        exact = []\n",
    "\n",
    "    # Generate an ideal natural-language summary using the summarization pipeline\n",
    "    ideal = sum_pipe(ctx, min_length=30, max_length=120,\n",
    "                     do_sample=False)[0][\"summary_text\"]\n",
    "\n",
    "    # Append the answer object to the final output list\n",
    "    answers_json.append({\n",
    "        \"id\": qid,\n",
    "        \"type\": qtype,\n",
    "        \"exact_answer\": exact,\n",
    "        \"ideal_answer\": ideal.strip()\n",
    "    })\n",
    "    \n",
    "# dump submission\n",
    "OUTFILE.write_text(json.dumps({\"questions\": answers_json}, indent=2))\n",
    "print(\"✅  Phase-A* answers written →\", OUTFILE)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
